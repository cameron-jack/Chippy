*****
Tasks
*****

- generate a genomic control in the following manner:
    - for each gene TSS select a +/- 10kb window and slide along (see slidingWindows method on cogent sequence objects) writing to a single file in fastq format (with maximum quality scores per base, possibly giving any N's a score of 3)
    - then map these to genome
    - generate the .npy counts files
- the region counts needs a class that load chromosome count data from multiple chromosomes
- the region counts needs a class that holds GeneIndex objects and, when given the treatment and control data, stores the combined counts. This object should also be able to produce the desired statistics (like the normalised difference; and jackknifed estimate on the variance etc ..)
- the segment_count.py drawing code needs to be modified to adjust for the above changes.

Looking at densities per repeat sequence
========================================

- suggest using the BLAT approach against a canonical (or consensus) reference sequence for the different repeat types (centromeric, telomeric, LINE etc ..)
- question for the centromeric/telomeric, do those repeat sequences occur outside of their prototypical locations? (answer to that probably by blasting using NCBI or Ensembl blast servers)

***************
Data properties
***************

Saturday, 05 June 2010

s6 file - 26963573 sequences
s7 file - 24755667 sequences
s8 file - 27669319 sequences

*******************
Directory Structure
*******************

All the original data that will be received in relation this project will be located in:
	
	/Volumes/Data/Users/tremethick/data

The data will be placed in separate directories, the name of the directory starting with the date of the run in the format YYMMDD
For example, the original data from the latest run can be found in:

	/Volumes/Data/Users/tremethick/data/100721_GAPC_0018_GERALD_26-07-2010_sbsuser
	
The data is divided into 'treatment' and 'control' directory depending on the nature of the experiment. This information will need to be provided by the lab, so that the data can be put in the appropriate folders.

Any results from the processing of the data can be found in a mirror of the data directory, with 'data' changed to 'results'. So for the 
data directory above, the results can be found in:

	/Volumes/Data/Users/tremethick/results/100721_GAPC_0018_GERALD_26-07-2010_sbsuser
	
The results directory contains the following directories (in order of processing of data) :

 *	trimmed - 		This contains both fasta and fastq formatted data files
					from the original data set after the sequences have been
					trimmed for bad base calls (Those with quality score 'B').
					If after trimming, the sequence is less than 35 bps long,
					it is discarded.
					trim_summary.txt has a summary of the number of sequences
					that were discarded from the original file.
				
 *	blat_output - 	blat is used to find the sequences that might be
					contaminated with adapter sequences. blat uses the trimmed
					fasta files found in the trimmed directory as its input.
					The output of blat (psl files) can be found in this
					directory. 
					
 *	sorted - 		This directory contains two sets of fastq files per
					original sequence file - those sequences which had no
					adapter sequences contamination are named as
					'name_pristine.fastq'. Those that were contaminated by
					adapter sequences, have their adapter sequences trimmed,
					and only kept if the sequence is longer than 35 bps. This
					process is carried out by the get_pristine_seqs.py
					script. 
					
 * bowtie_output -	The bowtie application is used to find alignments of the
					sequences in the sorted directory with a reference genome.
					The results of bowtie are saved in this directory.
					bowtie_summary.txt has information on the alignments that
					were found, per sequence file.This directory also has a
					concatenated version of the pristine and contaminated
					sequence results, as they can be used for useq analyses. 
					
 * useq_output - 	The useq ChIPSeq application is used to find peaks, using
					the bowtie output as input. The multiple outputs from useq
					are stored in this directory. 
					

A list of the possible adapters that could be contaminating the data files are stored in fasta format in

	/Volumes/Data/Users/tremethick/adatpters.fa

These are used in the blat part of the workflow to separate the pristine and contaminated sequences. 

The reference indices that are generated by bowtie can be found in

	/Volumes/Data/Users/tremethick/index/<species>
	
where <species> is the species that you are interested in. For eg. the mouse reference index can be found in

	/Volumes/Data/Users/tremethick/index/mouse


********
Workflow
********

N.B.
1.	The commands below assume that the ~cggroup/repos/lab/Chipseq/src is
	part of your path.
2.	Make sure that the original sequences and the results are put in the
	appropriate data directories as mentioned above. 
		 

1.	Plot quality calls for the data set.
 
	plot_quality.py -i <seqs.fastq> -o <qual_plot.png>
	
2.	Trim the bad bases from the sequence file, and create a fasta file which
	can be used as an input for blat to find the adapter sequences.
	
	fastq_to_fasta.py -i <seqs.fastq> -o <seqs.fasta> -q
	e.g. fastq_to_fasta.py -i $CHIPDATA/110203_GAPC_00034_FC_Data_Intensities_BaseCalls2_GERALD_17-02-2011_sbsuser/s_7_sequence.txt -o s_7.fasta
	
3.	Look for the adapters in the sequence using blat

	blat <adapter.fa> <seqs.fasta> <output.psl>
	e.g. blat $ADAPTERS s_7.fasta s_7.psl
	
	
4. 	Generate the pristine and contaminated fastq files (after removing the
	adapater sequences from the contaminated sequences)
	
	get_pristine_seqs.py -p <somefile.psl> -i <seqs> -o <outfile_root>
	get_pristine_seqs.py -p s_7.psl -i s_7_trimmed.fastq
	
5.	Map the reads to the genome using bowtie Do this for both the pristine and
	contaminated sequences and store their outputs separately.

	bowtie -q --solexa1.3-quals -t -m 1 -p 8 <index> <seq.fastq> <output.sam>
	e.g. $ run_bowtie $MOUSE_INDEX lap s_7_pristine.fastq s_7_pristine.map
	
6.	Concatenate the the pristine and contaminated mapped data for the next two
	steps. Note that this only works for default bowtie output. For sam output
	the header of the contaminated file has to be stripped prior to
	concatenation. 
	
	cat pristine.map contaminated.map > joined.map

7.	Generate the gzipped read frequency tables.
	
	minimal_reads.py -i ~/../tremethick/results/101028_GAPC_00026_FC_Data_Intensities_BaseCalls2_GERALD_01-11-2010_sbsuser/bowtie_output/s_8.map  -o LAP1

8.	Generate the coordinates of the matched regions for each chromosome based
	on the concatenated bowtie map data.
	
	region_reads.py -i <input_bowtie_file> -o <output_directory> -c chrom -w window_size

*************************
Running controller script
*************************

::

    $ gridsub -x fastq_to_mapped.py -a "--blat_adapters $ADAPTERS --bowtie_index $MOUSE_INDEX -i $CHIPDATA/110203_GAPC_00034_FC_Data_Intensities_BaseCalls2_GERALD_17-02-2011_sbsuser/s_8_sequence.txt -o s_8.fasta -S 30day_old_H2A.Z" -n gavin_full_30do
    $ gridsub -x fastq_to_mapped.py -a "--blat_adapters $ADAPTERS --bowtie_index $MOUSE_INDEX -i $CHIPDATA/101028_GAPC_00026_FC_Data_Intensities_BaseCalls2_GERALD_01-11-2010_sbsuser/s_8_sequence.txt -o s_8.fasta -S 30day_old_Lap1" -n gavin_full_30do_Lap1
    

Time taken: 3.5 hours; 3.8 hours

Doing 3 runs in one go:
6.66416666667
5.05472222222
5.65638888889

*****************************************
Refactor for inclusion of expression data
*****************************************

diff-file=M-less-G1-summTable.txt
diff-file=M-less-S-summTable.txt
diff-file=S-less-G1-summTable.txt
diff-file=DT-less-G1-summTable.txt
G1-file=G1.abs.exp.ranked.txt
M-file=M.abs.exp.ranked.txt
S-file=S.abs.exp.ranked.txt
DT-file=DT.abs.exp.ranked.txt


**********
References
**********

Fastq Format:
http://en.wikipedia.org/wiki/FASTQ_format
http://maq.sourceforge.net/fastq.shtml

Quality Scores:
http://docs.google.com/fileview?id=0B-lLYVUOliJFYjlkNjAwZjgtNDg4ZC00MTIyLTljNjgtMmUzN2M0NTUyNDE3&hl=en

Calculating Standard Deviation for large data sets"
http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance"
